{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0813 20:09:04.237091 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:31: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n",
      "W0813 20:09:04.237678 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:32: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "W0813 20:09:04.237991 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:33: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0813 20:09:04.436904 139962960201536 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0813 20:09:04.437405 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:35: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "W0813 20:09:04.437779 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/optimizers/tf_optimizer.py:36: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.\n",
      "\n",
      "W0813 20:09:04.440674 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:254: The name tf.layers.AveragePooling1D is deprecated. Please use tf.compat.v1.layers.AveragePooling1D instead.\n",
      "\n",
      "W0813 20:09:04.440998 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:255: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n",
      "\n",
      "W0813 20:09:04.441250 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:256: The name tf.layers.AveragePooling3D is deprecated. Please use tf.compat.v1.layers.AveragePooling3D instead.\n",
      "\n",
      "W0813 20:09:04.441494 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:257: The name tf.layers.BatchNormalization is deprecated. Please use tf.compat.v1.layers.BatchNormalization instead.\n",
      "\n",
      "W0813 20:09:04.441731 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:258: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import DQNAgent\n",
    "from tensorforce.execution import Runner\n",
    "from surface_env import *\n",
    "import ase.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energy(energy, ylabel, save_path):\n",
    "    plt.figure()\n",
    "    plt.xlabel('training episode')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title('episode vs. ' + ylabel)\n",
    "    plt.plot(energy)\n",
    "    plt.savefig(save_path)\n",
    "    print('figure saved as {}'.format(save_path))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=100000\n",
    "horizon=100\n",
    "deterministic=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 20:09:04.796082 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:252: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0813 20:09:04.796555 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:457: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0813 20:09:04.797903 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/explorations/exploration.py:42: The name tf.make_template is deprecated. Please use tf.compat.v1.make_template instead.\n",
      "\n",
      "W0813 20:09:04.818248 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:846: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0813 20:09:04.818682 139962960201536 deprecation.py:506] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:851: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 20:09:04.819022 139962960201536 deprecation_wrapper.py:119] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:852: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W0813 20:09:04.828251 139962960201536 deprecation.py:506] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 20:09:04.868032 139962960201536 deprecation.py:506] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n",
      "W0813 20:09:04.889778 139962960201536 deprecation.py:323] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0813 20:09:04.901452 139962960201536 deprecation.py:506] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:612: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 20:09:04.902016 139962960201536 deprecation.py:506] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/core/networks/layer.py:659: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0813 20:09:04.986948 139962960201536 deprecation.py:323] From /home/zulissi/miniconda3/envs/tensorforce/lib/python3.7/site-packages/tensorforce/models/model.py:1136: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial energy: 2.1797225799832667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.203453130743803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/100000 [00:04<130:43:19,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode 1 after 99 timesteps (reward: -1218.7341642328806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/100000 [00:12<154:17:09,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 5.6811491844662\n",
      "Finished episode 2 after 99 timesteps (reward: -4443.203246955412)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/100000 [00:16<140:46:35,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1875079314645465\n",
      "Finished episode 3 after 99 timesteps (reward: -82.04087422985182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/100000 [00:23<160:59:54,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.2864978392534514\n",
      "Finished episode 4 after 99 timesteps (reward: -1151.1087289120285)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 5/100000 [00:28<154:40:12,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.277389172563579\n",
      "Finished episode 5 after 99 timesteps (reward: -3625.377881347485)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 6/100000 [00:33<144:08:21,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 12.943111183990183\n",
      "Finished episode 6 after 99 timesteps (reward: -11622.287061279838)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 7/100000 [00:36<131:03:05,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.8125977761393246\n",
      "Finished episode 7 after 99 timesteps (reward: -3431.2214739502715)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 8/100000 [00:40<122:03:50,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.493426395182972\n",
      "Finished episode 8 after 99 timesteps (reward: -1381.1016530299858)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 9/100000 [00:44<122:43:33,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.3485182783211656\n",
      "Finished episode 9 after 99 timesteps (reward: -2331.3357348431655)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 10/100000 [00:49<126:23:06,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 13.713877313056651\n",
      "Finished episode 10 after 99 timesteps (reward: -12031.52542138605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 11/100000 [00:54<130:27:52,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.2678328461600934\n",
      "Finished episode 11 after 99 timesteps (reward: -1343.8391707526216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 12/100000 [00:59<129:11:55,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.3228914572984003\n",
      "Finished episode 12 after 99 timesteps (reward: -1440.3424458980617)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 13/100000 [01:04<131:33:36,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1788282454484627\n",
      "Finished episode 13 after 99 timesteps (reward: -48.45631671561708)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 14/100000 [01:11<151:02:44,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.179334259036505\n",
      "Finished episode 14 after 99 timesteps (reward: -657.7728868360778)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 15/100000 [01:15<141:45:04,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.286494863028242\n",
      "Finished episode 15 after 99 timesteps (reward: -2489.232419969305)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 16/100000 [01:19<134:36:30,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.188327376721487\n",
      "Finished episode 16 after 99 timesteps (reward: -1092.6066142686075)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 17/100000 [01:24<135:30:28,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.178928564906739\n",
      "Finished episode 17 after 99 timesteps (reward: -188.32214594555785)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 18/100000 [01:28<126:14:47,  4.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.2848599403151066\n",
      "Finished episode 18 after 99 timesteps (reward: -1155.1663520386605)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 19/100000 [01:32<118:25:20,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 5.738544457058266\n",
      "Finished episode 19 after 99 timesteps (reward: -4748.076814792938)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 20/100000 [01:36<123:15:56,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.7080918315362137\n",
      "Finished episode 20 after 99 timesteps (reward: -2084.081013066807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 21/100000 [01:40<116:05:20,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.180148664429808\n",
      "Finished episode 21 after 99 timesteps (reward: -1377.785500670012)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 22/100000 [01:44<118:05:28,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.7897952887762383\n",
      "Finished episode 22 after 99 timesteps (reward: -1925.0191984815312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 23/100000 [01:49<119:51:53,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1983369524573906\n",
      "Finished episode 23 after 99 timesteps (reward: -545.6723120853201)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 24/100000 [01:53<119:40:24,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 7.033445506642959\n",
      "Finished episode 24 after 99 timesteps (reward: -5495.852851761852)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 25/100000 [01:58<122:30:20,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.2998468308594244\n",
      "Finished episode 25 after 99 timesteps (reward: -1360.9672844150841)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 26/100000 [02:03<130:24:08,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1779076284937915\n",
      "Finished episode 26 after 99 timesteps (reward: -344.1297858174627)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 27/100000 [02:07<126:56:37,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 24.65352567065112\n",
      "Finished episode 27 after 99 timesteps (reward: -24289.75695094935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 28/100000 [02:11<118:09:21,  4.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1844567786233444\n",
      "Finished episode 28 after 99 timesteps (reward: -1949.8013158375272)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 29/100000 [02:16<127:00:06,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.830743338730894\n",
      "Finished episode 29 after 99 timesteps (reward: -3021.998532325596)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 30/100000 [02:21<128:57:57,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.294244128971629\n",
      "Finished episode 30 after 99 timesteps (reward: -197.571074241127)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 31/100000 [02:26<127:16:29,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.179850612785142\n",
      "Finished episode 31 after 99 timesteps (reward: -255.7460370371204)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 32/100000 [02:29<121:12:20,  4.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 27.198226120152768\n",
      "Finished episode 32 after 99 timesteps (reward: -25924.82605760337)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 33/100000 [02:34<127:06:01,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.1764694786187384\n",
      "Finished episode 33 after 99 timesteps (reward: -2164.8222164504914)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 34/100000 [02:38<118:16:26,  4.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 6.598390937522028\n",
      "Finished episode 34 after 99 timesteps (reward: -4839.348605099076)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 35/100000 [02:43<124:30:38,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1788612666917455\n",
      "Finished episode 35 after 99 timesteps (reward: -952.7894360523804)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 36/100000 [02:48<131:22:41,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.2989634341093588\n",
      "Finished episode 36 after 99 timesteps (reward: -845.0714680664379)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 37/100000 [02:53<128:04:46,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 6.166770354158556\n",
      "Finished episode 37 after 99 timesteps (reward: -5400.104495453827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 38/100000 [02:56<118:36:35,  4.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.1845057420905665\n",
      "Finished episode 38 after 99 timesteps (reward: -641.1916469988714)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 39/100000 [03:01<123:24:52,  4.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.198581466845271\n",
      "Finished episode 39 after 99 timesteps (reward: -858.4048844602773)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 40/100000 [03:06<128:19:23,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.197123121333812\n",
      "Finished episode 40 after 99 timesteps (reward: -400.4160726082769)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 41/100000 [03:12<137:29:46,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 22.42091259171094\n",
      "Finished episode 41 after 99 timesteps (reward: -20710.49396342371)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 42/100000 [03:16<129:42:08,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 14.592684542847987\n",
      "Finished episode 42 after 99 timesteps (reward: -12547.153672831373)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 43/100000 [03:21<134:38:26,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 3.9310592562964874\n",
      "Finished episode 43 after 99 timesteps (reward: -1950.532636394462)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 44/100000 [03:25<129:53:31,  4.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 2.182536380702908\n",
      "Finished episode 44 after 99 timesteps (reward: -176.90678013080816)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 45/100000 [03:30<129:01:27,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode ends and final energy: 5.734890622595401\n",
      "Finished episode 45 after 99 timesteps (reward: -5426.771972529091)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = SurfaceEnv(horizon)\n",
    "print('Initial energy:', env.get_energy())\n",
    "\n",
    "# lattice = Surface()\n",
    "# lattice.reset()\n",
    "# print(lattice.current_positions())\n",
    "# print(lattice.current_positions().shape)\n",
    "# print(type(lattice.current_positions()))\n",
    "\n",
    "network_spec = [\n",
    "    {\n",
    "        \"type\": \"dense\",\n",
    "        \"size\": 64,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"dense\",\n",
    "        \"size\": 32,\n",
    "        \"activation\": \"relu\"\n",
    "    }\n",
    "    # {\n",
    "    #     \"type\": \"dense\",\n",
    "    #     \"size\": 6+32,\n",
    "    #     \"activation\": \"softmax\"\n",
    "    # }\n",
    "]\n",
    "\n",
    "# print(env.states)\n",
    "# print(env.states['shape'])\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states=env.states,\n",
    "    actions=env.actions,\n",
    "    network=network_spec,\n",
    "    batched_observe=True, \n",
    "    batching_capacity=8000,\n",
    "    execution=dict(\n",
    "        type='single',\n",
    "        session_config=None,\n",
    "        distributed_spec=None\n",
    "    ), \n",
    "\n",
    "    states_preprocessing=None,\n",
    "    reward_preprocessing=None,\n",
    "\n",
    "    update_mode=dict(\n",
    "        unit='timesteps',\n",
    "        batch_size=10,\n",
    "        frequency=10\n",
    "    ),\n",
    "    memory=dict(\n",
    "        type='replay',\n",
    "        include_next_states=True,\n",
    "        capacity=40000\n",
    "    ),\n",
    "\n",
    "    optimizer=dict(\n",
    "        type='clipped_step',\n",
    "        clipping_value=0.1,\n",
    "        optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        )\n",
    "    ),\n",
    "    actions_exploration=dict(\n",
    "        type='epsilon_anneal',\n",
    "        initial_epsilon=0.5,\n",
    "        final_epsilon=0.05,\n",
    "        timesteps=1000000\n",
    "    ),\n",
    "    discount=1,\n",
    "    distributions=None,\n",
    "    entropy_regularization=0.01,\n",
    "    target_sync_frequency=1000,\n",
    "    target_update_weight=1.0,\n",
    "    double_q_model=False,\n",
    "    huber_loss=None,\n",
    "\n",
    "    summarizer=dict(\n",
    "        directory=None,\n",
    "        labels=['graph', 'total-loss']\n",
    "    ),\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=env,\n",
    "    repeat_actions=1\n",
    ")\n",
    "\n",
    "cur_min = -1e6\n",
    "\n",
    "def episode_finished(r):\n",
    "    \n",
    "    global cur_min\n",
    "    # positions = env.get_positions()\n",
    "    # print(\"pos diff:\", np.sum(positions[1,:] - positions[0,:]))\n",
    "    # np.save('shit', positions)\n",
    "    \n",
    "    if r.episode % 100 == 0 or r.episode_rewards[-1]>cur_min:\n",
    "        final_energy = env.get_energy()\n",
    "        positions = env.get_positions()\n",
    "        \n",
    "        image_list = []\n",
    "        with ase.io.trajectory.TrajectoryWriter('gifs/%d_%1.2f_%1.2f.traj'%(r.episode,final_energy, r.episode_rewards[-1])) as traj:\n",
    "            for pos in positions:\n",
    "                env._lattice.set_free_atoms(pos)\n",
    "                copy_atoms = env._lattice.atoms.copy()\n",
    "                image_list.append(copy_atoms.repeat((2,2,1)))\n",
    "                traj.write(copy_atoms, energy=env.get_energy())\n",
    "\n",
    "#         ase.io.write('gifs/%d_%1.2f.gif'%(r.episode,final_energy) , image_list, interval=100)\n",
    "        \n",
    "        if final_energy > cur_min:\n",
    "            cur_min = r.episode_rewards[-1]\n",
    "    if r.episode % 50 == 0:\n",
    "        positions = env.get_positions()\n",
    "        pos_fn = '_'.join(['pos_3', str(r.episode)])\n",
    "        pos_dir = os.path.join('new_pos', pos_fn)\n",
    "        np.save(pos_dir, positions)\n",
    "    if r.episode % 50 == 0:\n",
    "        agent_fn = '_'.join(['agent_3', str(r.episode)])\n",
    "        agent_dir = os.path.join('new_agents', agent_fn)\n",
    "        r.agent.save_model(agent_dir)\n",
    "        print(\"Saving agent to {}\".format(agent_dir))\n",
    "    if r.episode % 50 == 0:\n",
    "        rew_fn = '.'.join(['_'.join(['reward_3', str(r.episode)]), 'png'])\n",
    "        rew_dir = os.path.join('new_plots', rew_fn)\n",
    "        plot_energy(r.episode_rewards, 'accumulated reward', rew_dir)\n",
    "        energy_fn = '.'.join(['_'.join(['final', 'energy_3', str(r.episode)]), 'png'])\n",
    "        energy_dir = os.path.join('new_plots', energy_fn)\n",
    "        plot_energy(env.final_energy, 'final energy', energy_dir)\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".\n",
    "        format(ep=r.episode, ts=r.episode_timestep,reward=r.episode_rewards[-1]))\n",
    "    return True\n",
    "\n",
    "runner.run(\n",
    "    # num_timesteps=args.timesteps,\n",
    "    num_episodes=episodes,\n",
    "    max_episode_timesteps=horizon,\n",
    "    deterministic=deterministic,\n",
    "    episode_finished=episode_finished\n",
    ")\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
