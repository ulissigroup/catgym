{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0,'/home/junwoony/Desktop/ARPAE/')\n",
    "\n",
    "import argparse\n",
    "import importlib\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "import ase.io\n",
    "\n",
    "from env.surface_env import *\n",
    "# from utils.terminate import \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorforce import TensorForceError\n",
    "from tensorforce.agents import DQNAgent\n",
    "from tensorforce.execution import Runner\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial energy: 2.17972257998324\n"
     ]
    }
   ],
   "source": [
    "horizon=200\n",
    "episodes=10000\n",
    "env = SurfaceEnv(horizon)\n",
    "print('Initial energy:', env.get_energy())\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "tf.random.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "network_spec = [\n",
    "    {\n",
    "        \"type\": \"dense\",\n",
    "        \"size\": 64,\n",
    "        \"activation\": \"relu\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"dense\",\n",
    "        \"size\": 32,\n",
    "        \"activation\": \"relu\"\n",
    "    }\n",
    "]\n",
    "\n",
    "agent = DQNAgent(\n",
    "    states=env.states,\n",
    "    actions=env.actions,\n",
    "    network=network_spec,\n",
    "    batched_observe=True, \n",
    "    batching_capacity=8000,\n",
    "    execution=dict(\n",
    "        type='single',\n",
    "        session_config=None,\n",
    "        distributed_spec=None\n",
    "    ), \n",
    "\n",
    "    states_preprocessing=None,\n",
    "    reward_preprocessing=None,\n",
    "\n",
    "    update_mode=dict(\n",
    "        unit='timesteps',\n",
    "        batch_size=10,\n",
    "        frequency=10\n",
    "    ),\n",
    "    memory=dict(\n",
    "        type='replay',\n",
    "        include_next_states=True,\n",
    "        capacity=40000\n",
    "    ),\n",
    "\n",
    "    optimizer=dict(\n",
    "        type='clipped_step',\n",
    "        clipping_value=0.1,\n",
    "        optimizer=dict(\n",
    "            type='adam',\n",
    "            learning_rate=1e-3\n",
    "        )\n",
    "    ),\n",
    "    actions_exploration=dict(\n",
    "        type='epsilon_anneal',\n",
    "        initial_epsilon=0.5,\n",
    "        final_epsilon=0.05,\n",
    "        timesteps=1000000\n",
    "    ),\n",
    "    discount=1,\n",
    "    distributions=None,\n",
    "    entropy_regularization=0.01,\n",
    "    target_sync_frequency=1000,\n",
    "    target_update_weight=1.0,\n",
    "    double_q_model=False,\n",
    "    huber_loss=None,\n",
    "\n",
    "    summarizer=dict(\n",
    "        directory=None,\n",
    "        labels=['graph', 'total-loss']\n",
    "    ),\n",
    ")\n",
    "\n",
    "runner = Runner(\n",
    "    agent=agent,\n",
    "    environment=env,\n",
    "    repeat_actions=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energy(energy, xlabel, ylabel, save_path):\n",
    "    plt.figure()\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(xlabel+ ' vs. ' + ylabel)\n",
    "    plt.plot(energy)\n",
    "    plt.savefig(save_path)\n",
    "    print('figure saved as {}'.format(save_path))\n",
    "    return \n",
    "\n",
    "def episode_finished(r):\n",
    "    # if r.episode % 50 == 0:\n",
    "    #     positions = env.get_positions()\n",
    "    #     pos_fn = '_'.join(['pos_3', str(r.episode)])\n",
    "    #     pos_dir = os.path.join('new_pos', pos_fn)\n",
    "    #     np.save(pos_dir, positions)\n",
    "    # if r.episode % 50 == 0:\n",
    "    #     agent_fn = '_'.join(['agent_3', str(r.episode)])\n",
    "    #     agent_path = os.path.join('new_agents', agent_fn)\n",
    "    #     r.agent.save_model(agent_path)\n",
    "    #     print(\"Saving agent to {}\".format(agent_dir))\n",
    "    # if r.episode % 50 == 0:\n",
    "    #     rew_fn = '.'.join(['_'.join(['reward_3', str(r.episode)]), 'png'])\n",
    "    #     rew_dir = os.path.join('new_plots', rew_fn)\n",
    "    #     plot_energy(r.episode_rewards, 'accumulated reward', rew_dir)\n",
    "    #     energy_fn = '.'.join(['_'.join(['final', 'energy_3', str(r.episode)]), 'png'])\n",
    "    #     energy_dir = os.path.join('new_plots', energy_fn)\n",
    "    #     plot_energy(env.final_energy, 'final energy', energy_dir)\n",
    "\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".\n",
    "        format(ep=r.episode, ts=r.episode_timestep,reward=r.episode_rewards[-1]))\n",
    "\n",
    "    if r.episode % 1 == 0:\n",
    "        traj_dir = os.path.join('traj_files', 'seed_'+str(seed), str(r.episode))\n",
    "        if not os.path.exists(traj_dir):\n",
    "            os.makedirs(traj_dir)\n",
    "        env.save_traj(traj_dir)\n",
    "\n",
    "        E_dir = os.path.join('E_figs', 'seed_'+str(seed))\n",
    "        if not os.path.exists(E_dir):\n",
    "            os.makedirs(E_dir)\n",
    "        E_fn = 'E_' + str(r.episode) + '_%f' %env.ts_energy[-1] + '.png'            \n",
    "        E_fn = os.path.join(E_dir, E_fn)\n",
    "        plot_energy(env.energies, 'actions', 'energy', E_fn)\n",
    "#         fig_dir = os.path.join('atom_figs', 'seed_'+str(seed), str(r.episode))\n",
    "#         if not os.path.exists(fig_dir):\n",
    "#             os.makedirs(fig_dir)\n",
    "#         env.save_fig(fig_dir)\n",
    "\n",
    "    if r.episode % 50 == 0:\n",
    "        model_dir = os.path.join('models', 'seed_'+str(seed))\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        model_fn = os.path.join(model_dir, str(r.episode))\n",
    "        r.agent.save_model(model_fn)\n",
    "        print(\"Model saved to {}\".format(model_fn))\n",
    "\n",
    "        rew_dir = os.path.join('rew_figs', 'seed_'+str(seed))\n",
    "        if not os.path.exists(rew_dir):\n",
    "            os.makedirs(rew_dir)\n",
    "        rew_fn = 'rew_' + str(r.episode) + '.png'\n",
    "        rew_fn = os.path.join(rew_dir, rew_fn)\n",
    "        plot_energy(r.episode_rewards, 'training episode', 'accumulated reward', rew_fn)\n",
    "        energy_fn = 'final_energy_' + str(r.episode) + '.png'\n",
    "        energy_fn = os.path.join(rew_dir, energy_fn)\n",
    "        plot_energy(env.final_energy, 'training episode', 'final energy', energy_fn)\n",
    "        energy_fn = 'ts_energy_' + str(r.episode) + '.png'\n",
    "        energy_fn = os.path.join(rew_dir, energy_fn)\n",
    "        plot_energy(env.ts_energy, 'training episode', 'ts energy', energy_fn)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner.run(\n",
    "        num_episodes=episodes,\n",
    "        max_episode_timesteps=horizon,\n",
    "        deterministic=False,\n",
    "        episode_finished=episode_finished\n",
    "    )\n",
    "runner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
